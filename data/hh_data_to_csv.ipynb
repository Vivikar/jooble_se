{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "import config\n",
    "\n",
    "data_path = os.path.join(\"headHunter_data\")\n",
    "\n",
    "text_processing_url = config.text_processing_url\n",
    "\n",
    "pd.options.display.max_rows = 20\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(os.path.join(data_path, \"hh_ids.json\"), 'rb') as inf:\n",
    "    ids = json.load(inf)\n",
    "    \n",
    "with open(os.path.join(data_path, \"hh_vacancies.json\"), 'rb') as inf:\n",
    "    vacancies = json.load(inf)\n",
    "    \n",
    "with open(os.path.join(data_path, \"hh_vacancies_ext.json\"), 'rb') as inf:\n",
    "    vacancies_ext = json.load(inf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"id\", \"title\", \"specializations\", \"profarea_names\", \"requirement\", \n",
    "           \"requirement_norm\", \"responsibility_norm\", \"responsibility\",\n",
    "           \"url\", \"title_normalized\", \"text_raw\", \"text_normilized\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vac_rows = []\n",
    "\n",
    "for vac in vacancies:\n",
    "    row = {\"id\": vac[\"id\"], \"title\": vac[\"name\"],\n",
    "           \"title_normalized\": \"\",\n",
    "           \"lang_title\": \"\",\n",
    "           \"requirement_norm\": \"\",\n",
    "           \"responsibility_norm\": \"\",\n",
    "           \"requirement\": vac[\"snippet\"][\"requirement\"],\n",
    "           \"responsibility\": vac[\"snippet\"][\"responsibility\"],\n",
    "           \"url\": vac[\"url\"]}\n",
    "    vac_rows.append(row)   \n",
    "vac_df = pd.DataFrame(vac_rows)\n",
    "\n",
    "vac_rows = []\n",
    "for vac in vacancies_ext:\n",
    "    row = {\"id\": vac[\"id\"], \"text_raw\": vac[\"description\"],\n",
    "           \"text_normalized\": \"\", \"lang_text\": \"\",\n",
    "           \"specializations\": [i[\"name\"] for i in vac[\"specializations\"]],\n",
    "           \"profarea_names\": [i[\"profarea_name\"] for i in vac[\"specializations\"]]}\n",
    "    vac_rows.append(row)   \n",
    "vac_df_ext = pd.DataFrame(vac_rows)\n",
    "\n",
    "vac_df.drop_duplicates([\"id\"], inplace=True)\n",
    "vac_df_ext.drop_duplicates([\"id\"], inplace=True)\n",
    "full_df = vac_df.merge(vac_df_ext, left_on='id', right_on='id', how='outer')\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize text and title using text_preprocessing service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for index, row in full_df.iterrows():\n",
    "    title = row[\"title\"]\n",
    "    text = row[\"text_raw\"]\n",
    "    requirement = row[\"requirement\"]\n",
    "    responsibility = row[\"responsibility\"] \n",
    "    \n",
    "    r = requests.post(text_processing_url + config.STEM_TEXT_PATH,\n",
    "                          json=text)\n",
    "    full_df.loc[index, \"text_normalized\"] = r.text\n",
    "    \n",
    "    r = requests.post(text_processing_url + config.STEM_TEXT_PATH,\n",
    "                          json=title)\n",
    "    full_df.loc[index, \"title_normalized\"] = r.text\n",
    "    \n",
    "    r = requests.post(text_processing_url + config.STEM_TEXT_PATH,\n",
    "                          json=requirement)\n",
    "    full_df.loc[index, \"requirement_norm\"] = r.text\n",
    "    \n",
    "    r = requests.post(text_processing_url + config.STEM_TEXT_PATH,\n",
    "                          json=responsibility)\n",
    "    full_df.loc[index, \"responsibility_norm\"] = r.text\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(len(full_df))\n",
    "full_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.to_csv(os.path.join(data_path, \"hh_dataset.csv\"),\n",
    "               sep='\\t', header=True, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacancies[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacancies_ext[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
