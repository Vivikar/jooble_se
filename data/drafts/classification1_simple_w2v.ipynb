{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\My_programs\\Miniconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding, Bidirectional\n",
    "from keras.layers import SpatialDropout1D\n",
    "from gensim.models import Word2Vec\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.regularizers import l2\n",
    "import seaborn as sns\n",
    "\n",
    "import config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh_data_path = os.path.join(\"headHunter_data\", \"hh_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_hh = pd.read_csv(hh_data_path, sep=\"\\t\")\n",
    "dataset_jooble = pd.read_csv(\"by_jobs.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_hh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_jooble.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "spec_list = []\n",
    "for i, row in dataset_hh.iterrows():\n",
    "    specs = row[\"profarea_names\"].lower()\n",
    "    specs = specs.split(\"', \") \n",
    "    for spec in specs: \n",
    "        spec = re.sub('[\\[\\'\\]]', '', spec)\n",
    "        spec_list.append(spec.strip())\n",
    "    corpus.append(row[\"text_normalized\"].split(\" \"))\n",
    "    \n",
    "for i, row in dataset_jooble.loc[:5000].iterrows():\n",
    "    corpus.append(row[\"text_normalized\"].split(\" \"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14399\n",
      "Corpus size = 8654\n",
      "set of prof areas = 28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['управление персоналом, тренинги',\n",
       " 'начало карьеры, студенты',\n",
       " 'административный персонал',\n",
       " 'консультирование',\n",
       " 'закупки',\n",
       " 'автомобильный бизнес',\n",
       " 'безопасность',\n",
       " 'добыча сырья',\n",
       " 'наука, образование',\n",
       " 'высший менеджмент',\n",
       " 'продажи',\n",
       " 'транспорт, логистика',\n",
       " 'медицина, фармацевтика',\n",
       " 'информационные технологии, интернет, телеком',\n",
       " 'юристы',\n",
       " 'рабочий персонал',\n",
       " 'искусство, развлечения, масс-медиа',\n",
       " 'инсталляция и сервис',\n",
       " 'маркетинг, реклама, pr',\n",
       " 'производство',\n",
       " 'домашний персонал',\n",
       " 'строительство, недвижимость',\n",
       " 'государственная служба, некоммерческие организации',\n",
       " 'бухгалтерия, управленческий учет, финансы предприятия',\n",
       " 'спортивные клубы, фитнес, салоны красоты',\n",
       " 'туризм, гостиницы, рестораны',\n",
       " 'страхование',\n",
       " 'банки, инвестиции, лизинг']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(spec_list))\n",
    "print(\"Corpus size =\", len(corpus))\n",
    "spec_list = list(set(spec_list))\n",
    "print(\"set of prof areas =\", len(spec_list))\n",
    "spec_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = Word2Vec.load('word2vec_simple.bin')\n",
    "vectorizer = Word2Vec(corpus, min_count=5)\n",
    "print(vectorizer)\n",
    "vectorizer.save('word2vec_simple.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word = np.reshape(vectorizer.wv[\"свеж\"], (1, -1))\n",
    "cosine_similarity(vectorizer.wv[\"компан\", \"молок\"], word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(text, vectorizer, max_len):\n",
    "    \"\"\"\n",
    "    :param str text: normalized text\n",
    "    :param Word2Vec vectorizer:\n",
    "    :param int max_len:\n",
    "    :return np.array text_vect: of shape like (1, max_len, 100)\n",
    "    \"\"\"\n",
    "    text_vect = []\n",
    "    words = text.split(\" \")\n",
    "    for word in words:\n",
    "        try:\n",
    "            word_vec = vectorizer.wv[word]\n",
    "            text_vect.append(word_vec)\n",
    "        except KeyError:\n",
    "            None\n",
    "      \n",
    "    np.array(text_vect)\n",
    "    text_vect = np.reshape(text_vect, (1, -1, 100))\n",
    "    text_vect = sequence.pad_sequences(text_vect, maxlen=max_len, dtype='float')\n",
    "            \n",
    "    return text_vect\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = spec_list\n",
    "n_classes = len(labels)\n",
    "n_examples = len(dataset_hh)\n",
    "X = []\n",
    "Y = np.zeros((n_examples, n_classes))\n",
    "for i, row in dataset_hh.iterrows():\n",
    "    X.append([])\n",
    "    words = row[\"text_normalized\"].split(\" \")\n",
    "    for word in words:\n",
    "        try:\n",
    "            word_vec = vectorizer.wv[word]\n",
    "            X[i].append(word_vec)\n",
    "        except KeyError:\n",
    "            None\n",
    "    \n",
    "    specs = row[\"profarea_names\"].lower() \n",
    "    specs = specs.split(\"', \") \n",
    "    for spec in specs: \n",
    "        spec = re.sub('[\\[\\'\\]]', '', spec)\n",
    "        pos = labels.index(spec.strip())\n",
    "        Y[i][pos] = 1\n",
    "        \n",
    "assert(len(X) == len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max([len(i) for i in X])\n",
    "print(\"Max len =\", max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sequence.pad_sequences(X, maxlen=max_len, dtype='float')\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(np.argmax(Y, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset_hh\n",
    "del dataset_jooble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X\n",
    "del Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes(y_one_hot, labels, threshold=0.5):\n",
    "    \"\"\"\n",
    "    :param np.array y_one_hot: shape like (1, n_classes)\n",
    "    :param list of str labels:\n",
    "    :return list of str classes: \n",
    "    \"\"\"\n",
    "    classes = []\n",
    "    y_rounded = np.zeros(y_one_hot.shape)\n",
    "    y_rounded[y_one_hot > threshold] = 1\n",
    "    for i in range(len(labels)):\n",
    "        if y_rounded[0][i] == 1:\n",
    "            classes.append(labels[i])\n",
    "            \n",
    "    return classes\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weight = np.ones((n_classes,))\n",
    "sample_weight[12] = 4\n",
    "sample_weight[11] = 2\n",
    "sample_weight[7] = 2\n",
    "sample_weight[5] = 2\n",
    "sample_weight[3] = 2\n",
    "sample_weight = [list(sample_weight)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Bidirectional(layer=LSTM(10, dropout=0.3, recurrent_dropout=0.3),\n",
    "                        input_shape=(max_len, 100)))\n",
    "\n",
    "model.add(Dense(n_classes, activation=\"relu\", kernel_regularizer=l2(0.1)))\n",
    "model.add(Dense(n_classes, activation=\"sigmoid\", kernel_regularizer=l2(0.1)))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"],\n",
    "              loss_weights=sample_weight)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=64, \n",
    "          epochs=2,\n",
    "          validation_split=0.15,\n",
    "          verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scores = model.evaluate(X_test, Y_test, batch_size=64)\n",
    "print(\"Test accuracy: %.2f%%\" % (scores[1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check model on real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_jooble = pd.read_csv(\"by_jobs.csv\", sep=\"\\t\")\n",
    "dataset_jooble = pd.read_csv(hh_data_path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#for i in range(1005, 1010):\n",
    "index = 3005\n",
    "title = dataset_jooble.loc[index, \"title\"]\n",
    "title_norm = dataset_jooble.loc[index, \"title_normalized\"]\n",
    "vacancy = dataset_jooble.loc[index, \"text_raw\"]\n",
    "vacancy_norm = dataset_jooble.loc[index, \"text_normalized\"]\n",
    "true_label = dataset_jooble.loc[index, \"profarea_names\"]\n",
    "\n",
    "vacancy_vect = vectorize(vacancy_norm, vectorizer, max_len)\n",
    "\n",
    "predict = model.predict(vacancy_vect)\n",
    "result = get_classes(predict, labels, threshold=0.4)\n",
    "\n",
    "title_vect = vectorize(title_norm, vectorizer, max_len)\n",
    "\n",
    "predict_title = model.predict(title_vect)\n",
    "result_title = get_classes(predict_title, labels, threshold=0.45)\n",
    "\n",
    "    #if np.sum(np.round(predict)) > 0:\n",
    "     #   break\n",
    "print(\"List of classes:\", labels)\n",
    "print()\n",
    "print(\"Classes for vacancy:\", result)\n",
    "print(\"Classes for title:\", result_title)\n",
    "print(\"True labels:\", true_label)\n",
    "print(np.argmax(predict))\n",
    "#print(predict_title)\n",
    "print(\"Vacancy title:\", title)\n",
    "print()\n",
    "print(vacancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"class1_simple_weights.hdf5\")\n",
    "\n",
    "saved_model = model.to_json()\n",
    "with open(\"class1_simple.json\", \"w\") as json_file:\n",
    "    json_file.write(saved_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# define training data\n",
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "\t\t\t['this', 'is', 'the', 'second', 'sentence'],\n",
    "\t\t\t['yet', 'another', 'sentence'],\n",
    "\t\t\t['one', 'more', 'sentence'],\n",
    "\t\t\t['and', 'the', 'final', 'sentence']]\n",
    "# train model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "print(words)\n",
    "# access vector for one word\n",
    "print(model['sentence'])\n",
    "# save model\n",
    "model.save('model.bin')\n",
    "# load model\n",
    "new_model = Word2Vec.load('model.bin')\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
