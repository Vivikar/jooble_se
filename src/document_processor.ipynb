{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding, Bidirectional, Dropout\n",
    "from keras.layers import SpatialDropout1D, Conv1D, MaxPooling1D\n",
    "from gensim.models import Word2Vec\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.regularizers import l2\n",
    "import seaborn as sns\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.models import model_from_json\n",
    "from bs4 import BeautifulSoup\n",
    "import jsonpickle\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Bidirectional, Concatenate\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "\n",
    "from config_src import config\n",
    "from document import Document\n",
    "import text_processor\n",
    "\n",
    "pd.options.display.max_rows = 100\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_path = os.path.join(config.data_dir, \"word_embedding/ruwikiruscorpora-nobigrams_upos_skipgram_300_5_2018.vec.gz\")\n",
    "jooble_data_path = os.path.join(config.src_dir, \"labeled_jooble_data\", \"dataset_jooble_labeled.csv\")\n",
    "hh_data_path = os.path.join(config.headhunter_dir, \"hh_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_jooble = pd.read_csv(jooble_data_path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_hh = pd.read_csv(hh_data_path , sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_hh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_hh.drop_duplicates([\"text\"], inplace=True)\n",
    "print(\"Final size of dataset =\", len(dataset_hh))\n",
    "dataset_hh.reset_index(drop=True, inplace=True)\n",
    "dataset_hh.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_jooble.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_inverted_index(document, inverted_index):\n",
    "    \"\"\"Add the document to inverted index.\n",
    "    \n",
    "    :param Document document: document must be preprocessed \n",
    "    :param dict inverted_index:\n",
    "    \n",
    "    :return dict inverted_index: Updated with new document.\n",
    "    \"\"\"\n",
    "    tokens = document.text_normalized.split(\" \")\n",
    "    for token in tokens:\n",
    "        if token in inverted_index.keys():\n",
    "            inverted_index[token].append(document.id)\n",
    "        else:\n",
    "            inverted_index[token] = [document.id]\n",
    "    return inverted_index\n",
    "\n",
    "\n",
    "\n",
    "def create_index_from_dataframe(dataframe, forward_index={},\n",
    "                               inverted_index={}, documents_id=[]):\n",
    "    \n",
    "    dataset = dataframe\n",
    "    dataset.drop_duplicates([\"text\"], inplace=True)\n",
    "    dataset.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    for index, row in dataset.iterrows():\n",
    "        doc_id = row.loc[\"id\"]\n",
    "        title = row.loc[\"title\"]\n",
    "        text = row.loc[\"text\"]\n",
    "        lang = row.loc[\"lang_text\"]\n",
    "        title_normalized = row.loc[\"title_normalized\"]\n",
    "        text_normalized = row.loc[\"text_normalized\"]\n",
    "        url = row.loc[\"url\"]\n",
    "        \n",
    "        document = Document(doc_id, title, text, lang,\n",
    "                            title_normalized, text_normalized, url)\n",
    "        try:   \n",
    "            if str(document.id) not in documents_id:\n",
    "                documents_id.append(str(document.id))\n",
    "                forward_index[str(document.id)] = document\n",
    "                inverted_index = add_inverted_index(document, inverted_index)\n",
    "        except AttributeError:\n",
    "            print(\"Bad vacancy index =\", index)\n",
    "            continue\n",
    "\n",
    "    return forward_index, inverted_index, documents_id\n",
    "\n",
    "\n",
    "def save_index(path, forward_index, inverted_index, documents_id,\n",
    "               forward_file=\"forward_index\", \n",
    "               inverted_file=\"inverted_index\", \n",
    "               id_file=\"documents_id\"):\n",
    "    \n",
    "    \"\"\"Save index as json files\n",
    "    \n",
    "    :param str path: path to folder\n",
    "    :param dict forward_index: link to forward_index instance.\n",
    "    :param dict inverted_index: link to inverted_index instance.\n",
    "    :param list of str documents_id: link to documents_id  instance.\n",
    "    \n",
    "    :param str forward_file: file name for forward_index without extension.\n",
    "    :param str inverted_file: file name for inverted_index without extension.\n",
    "    :param str id_file: file name for documents_id without extension.\n",
    "    \"\"\"\n",
    "    \n",
    "    file_path = os.path.join(path, forward_file + \".json\")\n",
    "    with open(file_path, 'w', encoding='utf8') as outfile:\n",
    "        forward_index = jsonpickle.encode(forward_index)\n",
    "        json.dump(forward_index, outfile, ensure_ascii=False)\n",
    "\n",
    "    file_path = os.path.join(path, inverted_file + \".json\")\n",
    "    with open(file_path, 'w', encoding='utf8') as outfile:\n",
    "        inverted_index = jsonpickle.encode(inverted_index)\n",
    "        json.dump(inverted_index, outfile, ensure_ascii=False)\n",
    "\n",
    "    file_path = os.path.join(path, id_file + \".json\")\n",
    "    with open(file_path, 'w') as outfile:\n",
    "        documents_id = jsonpickle.encode(documents_id)\n",
    "        json.dump(documents_id, outfile)\n",
    "\n",
    "        \n",
    "def clean_html(text):\n",
    "    \"\"\"\n",
    "    Clean text from html tags\n",
    "    \n",
    "    :param str text:\n",
    "    :return str text:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = BeautifulSoup(text, \"html\").text\n",
    "    except:\n",
    "        print(\"Exception in  clean_html. NoneType argument.\")\n",
    "        return \"\"\n",
    "    \n",
    "    return text\n",
    "\n",
    "def create_index_from_dataframe_hh(dataframe, forward_index={},\n",
    "                               inverted_index={}, documents_id=[]):\n",
    "    \n",
    "    dataset = dataframe\n",
    "    dataset.drop_duplicates([\"text\"], inplace=True)\n",
    "    dataset.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    for index, row in dataset.iterrows():\n",
    "        doc_id = row.loc[\"id\"]\n",
    "        title = row.loc[\"title\"]\n",
    "        text = clean_html(row.loc[\"text\"])\n",
    "        lang = row.loc[\"lang_text\"]\n",
    "        title_normalized = row.loc[\"title_normalized\"]\n",
    "        text_normalized = row.loc[\"text_normalized\"]\n",
    "        url = row.loc[\"url\"]\n",
    "        requirement_normalized = str(row.loc[\"requirement_norm\"]) + str(row.loc[\"responsibility_norm\"])\n",
    "        \n",
    "        if isinstance(row[\"profarea_names\"], str):\n",
    "            labels_norm = []\n",
    "            labels = []\n",
    "            specs = row[\"profarea_names\"].lower()\n",
    "            specs = specs.split(\"', \") \n",
    "            for spec in specs: \n",
    "                spec = re.sub('[\\[\\'\\]]', '', spec)\n",
    "                labels.append(spec)\n",
    "                spec = text_processor.normalize_text(spec.strip())\n",
    "                labels_norm.append(spec)\n",
    "            \n",
    "            prof_area_normalized = \" \".join(labels_norm)\n",
    "            prof_area = \" \".join(labels)\n",
    "        else:\n",
    "            prof_area_normalized = \"\"\n",
    "            prof_area = \"\"\n",
    "            \n",
    "        document = Document(doc_id, title, text, lang,\n",
    "                            title_normalized, text_normalized, url,\n",
    "                            requirement_normalized, prof_area, prof_area_normalized)\n",
    "        try:   \n",
    "            if str(document.id) not in documents_id:\n",
    "                documents_id.append(str(document.id))\n",
    "                forward_index[str(document.id)] = document\n",
    "                inverted_index = add_inverted_index(document, inverted_index)\n",
    "        except AttributeError:\n",
    "            print(\"Bad vacancy index =\", index)\n",
    "            continue\n",
    "\n",
    "    return forward_index, inverted_index, documents_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "forward_index, inverted_index, documents_id = create_index_from_dataframe_hh(dataset_hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_index, inverted_index, documents_id = create_index_from_dataframe(dataset_jooble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(forward_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_hh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in dataset_hh.iterrows():\n",
    "    text = dataset_hh.loc[i, \"text\"]\n",
    "    dataset_hh.loc[i, \"text\"] = clean_html(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_hh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_hh.to_csv(os.path.join(config.headhunter_dir, \"hh_dataset_cleaned_html.csv\"),\n",
    "                  sep='\\t', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_index, inverted_index, documents_id = create_index_from_dataframe(dataset_hh,\n",
    "                                                                         forward_index=forward_index,\n",
    "                                                                         inverted_index=inverted_index,\n",
    "                                                                         documents_id=documents_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final index size:\", len(forward_index), \" \", len(inverted_index), \" \", len(documents_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_index(config.index_dir, forward_index, inverted_index, documents_id,\n",
    "               forward_file=\"forward_index\", \n",
    "               inverted_file=\"inverted_index\", \n",
    "               id_file=\"documents_id\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "def build_tfidf_from_index(save_tfidf_path): \n",
    "    \"\"\"\n",
    "    :param str save_tfidf_path:\n",
    "    :retunt TfidfVectorizer vectorizer:\n",
    "    \"\"\"\n",
    "    global corpus\n",
    "    for index, i in enumerate(documents_id):\n",
    "        try:\n",
    "            assert(isinstance(forward_index[i].text_normalized, str))\n",
    "            corpus.append(forward_index[i].text_normalized)\n",
    "        except AssertionError:\n",
    "            print(\"Bad document index =\", index, \" id =\", i)\n",
    "            continue\n",
    "    print(len(corpus))        \n",
    "    vectorizer = TfidfVectorizer(min_df=1, ngram_range=(1, 2), \n",
    "                                lowercase=False)\n",
    "    vectorizer.fit(corpus)\n",
    "    \n",
    "    save_tfidf_path = os.path.join(save_tfidf_path, \"vectorizer_tfidf.dat\")\n",
    "    with open(save_tfidf_path, \"wb\") as ouf:\n",
    "        pickle.dump(vectorizer, ouf)          \n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_tfidf = build_tfidf_from_index(config.index_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_index[\"-4974176290193358031\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorizer_tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer_tfidf.get_feature_names()[130000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 200\n",
    "\n",
    "num_encoder_tokens = 300\n",
    "latent_dim = 400\n",
    "\n",
    "num_decoder_tokens = 2\n",
    "max_decoder_seq_length = max_len\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vectorizer = KeyedVectors.load_word2vec_format(w2v_path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_filename, model_weights_filename):\n",
    "    with open(model_filename, 'r', encoding='utf8') as f:\n",
    "        model = model_from_json(f.read())\n",
    "    model.load_weights(model_weights_filename)\n",
    "    return model\n",
    "\n",
    "encoder_model = load_model('encoder_model.json', 'encoder_model_weights.h5')\n",
    "decoder_model = load_model('decoder_model.json', 'decoder_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, 0] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        \"\"\"sampled_char = reverse_target_char_index[sampled_token_index]\"\"\"\n",
    "        decoded_sentence.append(sampled_token_index)\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (len(decoded_sentence) == max_decoder_seq_length) or \\\n",
    "        (len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 200\n",
    "def get_sample_nonlabeled(row):\n",
    "\n",
    "    text = row[\"text_lemmas_tags\"]\n",
    "    x_sample = get_training_sample(vectorizer, text)\n",
    "    x_sample = sequence.pad_sequences([x_sample], maxlen=max_len, dtype='float', padding=\"post\", truncating=\"post\")\n",
    "    x_sample = np.array(x_sample)\n",
    "  \n",
    "      #print(\"x shape =\", x_sample.shape)\n",
    "  \n",
    "    return x_sample\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_sample(model, text):\n",
    "    \"\"\"\n",
    "    Gets 1 sample of training data\n",
    "    \n",
    "    :param  model text:\n",
    "    :return str text:\n",
    "    \"\"\"\n",
    "    tagged_list = text.split(\" \")\n",
    "\n",
    "    vec_list = []\n",
    "    \n",
    "    #converting word2vec \n",
    "    for word in tagged_list:\n",
    "        try:\n",
    "            vec_list.append(model[word])\n",
    "        except:\n",
    "            #print(\"Word \" + word + \" isn't in vocab. Embeding as zeros\")\n",
    "            vec_list.append(np.zeros(300))\n",
    "    return vec_list\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "index = 8001\n",
    "\n",
    "x = get_sample_nonlabeled(dataset_jooble.loc[index])\n",
    "\n",
    "predict = decode_sequence(x)\n",
    "\n",
    "print(np.sum(predict))\n",
    "\n",
    "print(dataset_jooble.loc[index, \"title\"])\n",
    "print()\n",
    "print(dataset_jooble.loc[index, \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(predict))\n",
    "words = dataset_jooble.loc[index, \"text_lemmas\"].split(\" \")\n",
    "words2 = dataset_jooble.loc[index, \"text_normalized\"].split(\" \")\n",
    "for i in range(0, 50):\n",
    "    if len(words) > i:\n",
    "        word = words[i]\n",
    "        word2 = words2[i]\n",
    "    else:\n",
    "        word = \"END\"\n",
    "    print(i, 'Predicted', predict[i], \" - \", word, \" - \", word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_jooble = pd.read_csv(jooble_data_path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_jooble.loc[2000: 2100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "n_test = 8349\n",
    "#predicts = []\n",
    "count = 0\n",
    "\n",
    "for i, row in dataset_jooble.loc[8349: ].iterrows():\n",
    "    try:\n",
    "        x = get_sample_nonlabeled(dataset_jooble.loc[i])\n",
    "        predict = decode_sequence(x)\n",
    "        \n",
    "        tokens = dataset_jooble.loc[i, \"text_normalized\"].split(\" \")\n",
    "        req = []\n",
    "        for j, token in enumerate(tokens):\n",
    "            if j >= len(predict):\n",
    "                break\n",
    "            if predict[j] == 1:\n",
    "                count += 1\n",
    "                req.append(token)\n",
    "    \n",
    "        dataset_jooble.loc[i, \"requirement_normalized\"] =  \" \".join(req)\n",
    "        \n",
    "        #predicts.append({\"index\": i, \"id\": dataset_jooble.loc[i, \"id\"], \"predict\": x,\n",
    "        #                \"requirement\": \" \".join(req)})\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            file_name = \"dataset_jooble_labeled.csv\"\n",
    "            save_path = os.path.join(config.src_dir, \"labeled_jooble_data\", file_name)\n",
    "            dataset_jooble.to_csv(save_path, sep='\\t', header=True, index=False)\n",
    "            \n",
    "            #file_name = \"dataset_jooble_predicts_\" + str(i) + \".dat\"\n",
    "            #save_path = os.path.join(config.src_dir, \"labeled_jooble_data\", file_name)\n",
    "            #with open(save_path, \"wb\") as ouf:\n",
    "            #    pickle.dump(predicts, ouf)\n",
    "            #\n",
    "            print(i, \"Saved\")\n",
    "            print(\"Count =\" , count)\n",
    "            count = 0\n",
    "    except AttributeError:\n",
    "        print(\"Some error, index =\", i, \"id =\", dataset_jooble.loc[i, \"id\"])\n",
    "        print(\"Count =\" , count)\n",
    "        print(dataset_jooble.loc[i, \"text\"])\n",
    "        print()\n",
    "        dataset_jooble.loc[i, \"requirement_normalized\"] =  \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_jooble.loc[8300: 8400, \"text_lemmas_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_jooble.loc[8397, \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"dataset_jooble_labeled_all.csv\"\n",
    "save_path = os.path.join(config.src_dir, \"labeled_jooble_data\", file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_jooble.to_csv(save_path, sep='\\t', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
