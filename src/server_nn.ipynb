{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pymystem3\n",
    "#!pip install future\n",
    "#!pip install beautifulsoup4\n",
    "#!pip install ufal.udpipe\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from future import standard_library\n",
    "import sys\n",
    "import requests\n",
    "from pymystem3 import Mystem\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('russian')\n",
    "stop_words.extend(['что', 'это', 'так', 'вот', 'быть', 'как', 'в', '—', 'к', 'на'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting mapping for different tag systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/akutuzov/universal-pos-tags/4653e8a9154e93fe2f417c7fdb7a357b7d6ce333/ru-rnc.map'\n",
    "\n",
    "mapping = {}\n",
    "r = requests.get(url, stream=True)\n",
    "for pair in r.text.split('\\n'):\n",
    "    pair = re.sub('\\s+', ' ', pair, flags=re.U).split(' ')\n",
    "    if len(pair) > 1:\n",
    "        mapping[pair[0]] = pair[1]\n",
    "\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uploading pretrained word2vec russian model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import gensim\n",
    "\n",
    "# online loading\n",
    "#ruscorpora_model = api.load(\"word2vec-ruscorpora-300\")\n",
    "\n",
    "#local upload\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('ruwikiruscorpora_upos_skipgram_300_2_2018.vec.gz', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and preprocessing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/hh_dataset.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_raw\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getridoff_stopwords(text):\n",
    "    res  = []\n",
    "    for word in split_text(text):\n",
    "        if word not in stop_words:\n",
    "            res.append(word)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_mystem(text='Текст нужно передать функции в виде строки!', postags=True):\n",
    "    '''\n",
    "    Return list of taged and lemmed words\n",
    "    \n",
    "    :param str input string\n",
    "    :return list of strings lemmed and tagged words\n",
    "    '''\n",
    "    m = Mystem()\n",
    "    processed = m.analyze(text)\n",
    "    tagged = []\n",
    "    for w in processed:\n",
    "        try:\n",
    "            if w[\"analysis\"][0][\"lex\"] not in stop_words:\n",
    "                lemma = w[\"analysis\"][0][\"lex\"].lower().strip()\n",
    "                pos = w[\"analysis\"][0][\"gr\"].split(',')[0]\n",
    "                pos = pos.split('=')[0].strip()\n",
    "                if pos in mapping:\n",
    "                    tagged.append(lemma + '_' + mapping[pos]) # здесь мы конвертируем тэги\n",
    "                else:\n",
    "                    tagged.append(lemma + '_X') # на случай, если попадется тэг, которого нет в маппинге\n",
    "        except KeyError:\n",
    "            continue # я здесь пропускаю знаки препинания, но вы можете поступить по-другому\n",
    "    if not postags:\n",
    "        tagged = [t.split('_')[0] for t in tagged]\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(text):\n",
    "    \"\"\"\n",
    "    Clean text from html tags\n",
    "    \n",
    "    :param str text:\n",
    "    :return str text:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = BeautifulSoup(text, \"html\").text\n",
    "    except:\n",
    "        print(\"Exception in  clean_html. NoneType argument.\")\n",
    "        return \"\"\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text):\n",
    "    return(re.findall(r\"[\\w']+\", text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(text, req):\n",
    "    mask = []\n",
    "    # FOR TRAINING PURPOSES\n",
    "    # SHOULD HAVE BEEN DONE PREVIOUSLY!\n",
    "    # WHILE CREATING A DATASET\n",
    "    #print(tag_mystem(text, postags=False), tag_mystem(req, postags=False))\n",
    "    \n",
    "    text = tag_mystem(text, postags=False)\n",
    "    req = tag_mystem(req, postags=False)\n",
    "    \n",
    "    \n",
    "    i = 0\n",
    "    while i  <= (len(text)):\n",
    "        if (req == text[i:i+len(req)]):\n",
    "            for j in range(len(req)):\n",
    "                mask.append(1.)\n",
    "                i += 1\n",
    "        else:\n",
    "            mask.append(0.)\n",
    "        i += 1\n",
    "    return mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_sample(model, text, query):\n",
    "    \"\"\"\n",
    "    Gets 1 sample of training data\n",
    "    \n",
    "    :param  model text:\n",
    "    :return str text:\n",
    "    \"\"\"\n",
    "    tagged_list = tag_mystem(text)\n",
    "    mask = get_mask(text, query)\n",
    "    vec_list = []\n",
    "    \n",
    "    #converting word2vec \n",
    "    for word in tagged_list:\n",
    "        try:\n",
    "            vec_list.append(model[word])\n",
    "        except:\n",
    "            print(\"Word \" + word + \" isn't in vocab. Embeding as zeros\")\n",
    "            vec_list.append(np.zeros(300))\n",
    "    return vec_list, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vvec_list, mmask = get_training_sample(ruscorpora_model, \"Мартышка бежала по берегу наполненому змеями\", \"бежала по берегу\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(vvec_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(mmask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.zeros(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = df[:1][\"requirement\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = df[:1][\"text_raw\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = clean_html(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementin Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "from numpy import array\n",
    "from numpy import cumsum\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sequence of classification instance\n",
    "def get_sequence(model, text, query, n_timesteps=500, dims=300):\n",
    "    \"\"\"\n",
    "    Gets 1 sequence of training data fully prepared for NN\n",
    "    \n",
    "    :param  model text query\n",
    "    n_timesteps == max_num of words in one document\n",
    "    :return X y:\n",
    "    \"\"\"\n",
    "    X, y  = get_training_sample(model, text, query)\n",
    "\n",
    "    if len(X) < n_timesteps:\n",
    "        X = np.array(X)\n",
    "        X = np.concatenate((X, np.zeros((n_timesteps - len(X), dims))))\n",
    "        \n",
    "        y = np.array(y).reshape((len(y),1))\n",
    "        y = np.concatenate((y.reshape((len(y),1)), np.zeros((n_timesteps - len(y), 1))), axis=0)\n",
    "        \n",
    "    X = X.reshape(1, n_timesteps, dims)\n",
    "    y = y.reshape(1, n_timesteps, 1)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should try using KERAS EMBEDING LAYER instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_sequence(ruscorpora_model, \"Лужа и курица пошли гулять\", \"курица пошли\", n_timesteps=500, dims=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dimensionality of each vord-vector\n",
    "dims = 300\n",
    "# number of observations/samples/max words in one document\n",
    "n_timesteps = 5\n",
    "# dimensionality of the output space *2 for bidirectional\n",
    "output_dim = 500*2\n",
    "\n",
    "# define LSTM\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(output_dim, return_sequences=True), input_shape=(n_timesteps, dims)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "# train LSTM\n",
    "for epoch in range(2):\n",
    "    # generate new random sequence\n",
    "    X,y = get_sequence(n_timesteps, dims)\n",
    "    # fit model for one epoch on this sequence\n",
    "    model.fit(X, y, epochs=1, batch_size=1, verbose=2)\n",
    "\n",
    "# evaluate LSTM\n",
    "X,y = get_sequence(n_timesteps,  dims)\n",
    "yhat = model.predict_classes(X, verbose=0)\n",
    "for i in range(n_timesteps):\n",
    "    print('Expected:', y[0, i], 'Predicted', yhat[0, i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
